{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn import preprocessing\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('1429_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data=data[data['categories'].str.contains('Laptop')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the second Kindle I\\'ve purchased, and I love my Kindle! What I don\\'t like is that every time a device is \"upgraded,\" the upgrades are annoying. On this Kindle, the \"home\" page is changed - it lists My Reading List and My Library. So you have to press My Library to choose a new book. Then it\\'s a list of the books. Why couldn\\'t they just leave the home page showing the small pictures of the books you bought? I have no idea what My Reading List is - it lists books I\\'ve never heard of, by authors and topics of which I have no interest. Still, I would recommend a Kindle to anyone who reads books. The screen is easy to read, you can make the fonts bigger or smaller. I don\\'t feel the battery lasts any longer than my old one did, but it\\'s easy to charge. It\\'s thin and lightweight, and I pretty much carry my Kindle wherever I go. Anyone who buys a Kindle will definitely NOT be disappointed!!!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data['reviews.text'][17464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading List\n",
      "Reading List\n"
     ]
    }
   ],
   "source": [
    "doc=nlp('This is the second Kindle I\\'ve purchased, and I love my Kindle! What I don\\'t like is that every time a device is \"upgraded,\" the upgrades are annoying. On this Kindle, the \"home\" page is changed - it lists My Reading List and My Library. So you have to press My Library to choose a new book. Then it\\'s a list of the books. Why couldn\\'t they just leave the home page showing the small pictures of the books you bought? I have no idea what My Reading List is - it lists books I\\'ve never heard of, by authors and topics of which I have no interest. Still, I would recommend a Kindle to anyone who reads books. The screen is easy to read, you can make the fonts bigger or smaller. I don\\'t feel the battery lasts any longer than my old one did, but it\\'s easy to charge. It\\'s thin and lightweight, and I pretty much carry my Kindle wherever I go. Anyone who buys a Kindle will definitely NOT be disappointed!!!')\n",
    "l=[]\n",
    "\n",
    "for i in range(0,len(doc)):\n",
    "        if doc[i].pos_=='PROPN' and doc[i+1].pos_=='NOUN' :\n",
    "            print(doc[i],doc[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Cequa have better insurance coverage\")\n",
    "aspects=[]\n",
    "id=''\n",
    "for i in range(0,len(doc)-1):\n",
    "        if (doc[i].pos_=='PROPN' and doc[i+1].pos_=='NOUN') or(doc[i].pos_=='ADJ' and doc[i+1].pos_=='PROPN')or(doc[i].text=='the' and doc[i+1].pos_=='NOUN')or(doc[i].pos_=='NOUN' and doc[i+1].pos_=='NOUN')or(doc[i].pos_=='PROPN' and doc[i+1].pos_=='PROPN'):\n",
    "            id=str(doc[i])+' '+str(doc[i+1])\n",
    "            aspects.append(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insurance coverage']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[4].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(f_data['reviews.text'][17464])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is the second Kindle I've purchased, and I love my Kindle! What I don't like is that every time a device is \"upgraded,\" the upgrades are annoying. On this Kindle, the \"home\" page is changed - it lists My Reading List and My Library. So you have to press My Library to choose a new book. Then it's a list of the books. Why couldn't they just leave the home page showing the small pictures of the books you bought? I have no idea what My Reading List is - it lists books I've never heard of, by authors and topics of which I have no interest. Still, I would recommend a Kindle to anyone who reads books. The screen is easy to read, you can make the fonts bigger or smaller. I don't feel the battery lasts any longer than my old one did, but it's easy to charge. It's thin and lightweight, and I pretty much carry my Kindle wherever I go. Anyone who buys a Kindle will definitely NOT be disappointed!!!"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "findall() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8352\\1703324725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'NOUN'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[{n}{m}]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: findall() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "doc=nlp(f_data['reviews.text'][17464])\n",
    "l=[]\n",
    "for tok in doc:\n",
    "    if tok.pos_=='NOUN' :\n",
    "        l.append(re.findall(r\"[{n}{m}]\".format(n=tok,m=tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle\n"
     ]
    }
   ],
   "source": [
    "for c in nlp('not'):\n",
    "    print(spacy.explain(c.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cequa  |  PROPN  |  proper noun\n",
      "have  |  VERB  |  verb\n",
      "better  |  ADJ  |  adjective\n",
      "insurance  |  NOUN  |  noun\n",
      "coverage  |  NOUN  |  noun\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Cequa have better insurance coverage\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 7.37MB/s]                    \n",
      "2022-11-21 12:39:11 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-11-21 12:39:12 INFO: File exists: C:\\Users\\saifalikhan\\stanza_resources\\en\\default.zip\n",
      "2022-11-21 12:39:17 INFO: Finished downloading models and saved to C:\\Users\\saifalikhan\\stanza_resources.\n",
      "2022-11-21 12:39:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 11.0MB/s]                    \n",
      "2022-11-21 12:39:21 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-11-21 12:39:21 INFO: Use device: cpu\n",
      "2022-11-21 12:39:21 INFO: Loading: tokenize\n",
      "2022-11-21 12:39:22 INFO: Loading: pos\n",
      "2022-11-21 12:39:22 INFO: Loading: lemma\n",
      "2022-11-21 12:39:22 INFO: Loading: depparse\n",
      "2022-11-21 12:39:22 INFO: Loading: sentiment\n",
      "2022-11-21 12:39:23 INFO: Loading: constituency\n",
      "2022-11-21 12:39:23 INFO: Loading: ner\n",
      "2022-11-21 12:39:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "def aspect_opinion_mining(para):\n",
    "    sent_aspects=[]\n",
    "    parag_aspects=[]\n",
    "    for sen in para.split('.'): \n",
    "        important = nlp(sen)\n",
    "        descriptive_item = ''\n",
    "        target = ''\n",
    "        added_terms=''\n",
    "        for sent in important.sentences:\n",
    "            for wrd in sent.words:\n",
    "                if ((wrd.deprel == 'root' and wrd.pos == 'NOUN') or (wrd.deprel == 'nsubj' and wrd.pos == 'NOUN') or(wrd.deprel == 'nsubj' and wrd.pos == 'PROPN')):\n",
    "                    target = wrd.text\n",
    "                if wrd.pos=='ADV' and wrd.deprel =='advmod':\n",
    "                    added_terms = wrd.text\n",
    "                if wrd.pos == 'ADJ':\n",
    "                    descriptive_item = added_terms +' '+ wrd.text                 \n",
    "                if target!='' and descriptive_item!='':\n",
    "                    sent_aspects.append({'aspect': target,'opinion': descriptive_item})\n",
    "                    descriptive_item = ''\n",
    "                    target = ''\n",
    "                    added_terms=''\n",
    "    if(sent_aspects!=[]):\n",
    "        parag_aspects.append(sent_aspects)\n",
    "        sent_aspects=[]\n",
    "\n",
    "    paragraph,sentence=[],[]\n",
    "    for para in parag_aspects:\n",
    "        for asp in para:\n",
    "    #         print(asp)\n",
    "            label = '-'.join(asp.values())\n",
    "            sentence.append(label)\n",
    "        paragraph.extend(sentence)\n",
    "        sentence=[]\n",
    "    combined_asp_op = \",\".join(paragraph)\n",
    "    return combined_asp_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kindle- second,upgrades- annoying,List-just small,screen- easy,battery-longer old,one- easy'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_opinion_mining(f_data['reviews.text'][17464])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elon- good'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_opinion_mining('elon is a good CEO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect term extraction and sentiment classification via PyABSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "import git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AspectExtractor' from 'pyabsa.tasks.AspectTermExtraction.prediction.aspect_extractor' (c:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8352\\294036188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mavailable_checkpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavailable_checkpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_item\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDatasetItem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsa_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_absa_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_ABSA_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsa_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsa_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate_inference_set_for_apc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_apc_set_to_atepc_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_manager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload_all_available_datasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_dataset_by_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\utils\\absa_utils\\make_absa_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelPaddingOption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspect_extractor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAspectExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor_template\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInferenceModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCModelList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lcf__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matepc_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_atepc_inference_datasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess_iob_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lcf__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils_for_inference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCProcessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_ate_examples_to_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_apc_examples_to_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__lcf__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils_for_training\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_aspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# for Aspect-term Extraction and Sentiment Classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matepc_trainer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCTrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matepc_configuration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCConfigManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCModelList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\trainer\\atepc_trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDeviceTypeOption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelSaveOption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTaskCodeOption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTaskNameOption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matepc_configuration\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCConfigManager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maspect_extractor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAspectExtractor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAspectTermExtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstructor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matepc_instructor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mATEPCTrainingInstructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyabsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer_template\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AspectExtractor' from 'pyabsa.tasks.AspectTermExtraction.prediction.aspect_extractor' (c:\\Users\\saifalikhan\\OneDrive - OPTIMAL STRATEGIX GROUP INC\\Desktop\\Monkey_learn\\.monlvenv\\lib\\site-packages\\pyabsa\\tasks\\AspectTermExtraction\\prediction\\aspect_extractor.py)"
     ]
    }
   ],
   "source": [
    "from pyabsa import available_checkpoints\n",
    "checkpoint_map = available_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('.monlvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "556f1f47c1653f9a2a366432905d9409fce0f42d54a14ea4bc9672727de24e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
